
Coursera Data Science Final Project
This project was completed for the Capstone Data Sceince course as part of the IBM Data Science Specialization.

Instructions
Now that you have been equipped with the skills and the tools to use location data to explore a geographical location, over the course of two weeks, you will have the opportunity to be as creative as you want and come up with an idea to leverage the Foursquare location data to explore or compare neighborhoods or cities of your choice or to come up with a problem that you can use the Foursquare location data to solve. If you cannot think of an idea or a problem, here are some ideas to get you started:

1) In Module 3, we explored New York City and the city of Toronto and segmented and clustered their neighborhoods. Both cities are very diverse and are the financial capitals of their respective countries. One interesting idea would be to compare the neighborhoods of the two cities and determine how similar or dissimilar they are. Is New York City more like Toronto or Paris or some other multicultural city? I will leave it to you to refine this idea.

2) In a city of your choice, if someone is looking to open a restaurant, where would you recommend that they open it? Similarly, if a contractor is trying to start their own business, where would you recommend that they setup their office?

These are just a couple of many ideas and problems that can be solved using location data in addition to other datasets. No matter what you decide to do, make sure to provide sufficient justification of why you think what you want to do or solve is important and why would a client or a group of people be interested in your project.

 A description of the problem and a discussion of the background.
An individual wishes to move to London, however he has never lived in a large city before and wishes to live in a quieter borough of London. He is a freelance writer and likes to write in local coffee shops, thus it is important he lives in an area with lots of coffee shops to choose from. This project will be focus on helping to find the best borough of London for the individual to live in.

A description of the data and how it will be used to solve the problem.
In order to find data for this project, we will scrape data from the table on the webpage: https://en.wikipedia.org/wiki/List_of_London_boroughs. This contains information on all 32 London boroughs. For this project we are interested in the name, location, area and population of the borough. The location of each borough will allow us to make a Foursquare call to identify the number of coffee shops in each borough. The size and population of each borough will allow us to calculate the population density, this will be used to identify how busy the area is.

Data Preperation
First we import the relevant packages.

In [3]:
import pandas as pd
from pandas.io.html import read_html

import requests
import numpy as np
from geopy.geocoders import Nominatim 
from pandas.io.json import json_normalize  
import folium 
from sklearn.cluster import KMeans

import matplotlib.pyplot as plt
import matplotlib.cm as cm
import matplotlib.colors as colors
First we scarpe the data and create a dataframe

In [4]:
url = 'https://en.wikipedia.org/wiki/List_of_London_boroughs'
wikipages = read_html(url , attrs = {'class' : 'wikitable'})

df = pd.DataFrame(wikipages[0] , columns=['Borough' , 'Co-ordinates' , 'Area (sq mi)' , 'Population (2013 est)[1]'])
df.head(32)
Out[4]:
Borough	Co-ordinates	Area (sq mi)	Population (2013 est)[1]
0	Barking and Dagenham [note 1]	51°33′39″N 0°09′21″E﻿ / ﻿51.5607°N 0.1557°E	13.93	194352
1	Barnet	51°37′31″N 0°09′06″W﻿ / ﻿51.6252°N 0.1517°W	33.49	369088
2	Bexley	51°27′18″N 0°09′02″E﻿ / ﻿51.4549°N 0.1505°E	23.38	236687
3	Brent	51°33′32″N 0°16′54″W﻿ / ﻿51.5588°N 0.2817°W	16.70	317264
4	Bromley	51°24′14″N 0°01′11″E﻿ / ﻿51.4039°N 0.0198°E	57.97	317899
5	Camden	51°31′44″N 0°07′32″W﻿ / ﻿51.5290°N 0.1255°W	8.40	229719
6	Croydon	51°22′17″N 0°05′52″W﻿ / ﻿51.3714°N 0.0977°W	33.41	372752
7	Ealing	51°30′47″N 0°18′32″W﻿ / ﻿51.5130°N 0.3089°W	21.44	342494
8	Enfield	51°39′14″N 0°04′48″W﻿ / ﻿51.6538°N 0.0799°W	31.74	320524
9	Greenwich [note 2]	51°29′21″N 0°03′53″E﻿ / ﻿51.4892°N 0.0648°E	18.28	264008
10	Hackney	51°32′42″N 0°03′19″W﻿ / ﻿51.5450°N 0.0553°W	7.36	257379
11	Hammersmith and Fulham [note 4]	51°29′34″N 0°14′02″W﻿ / ﻿51.4927°N 0.2339°W	6.33	178685
12	Haringey	51°36′00″N 0°06′43″W﻿ / ﻿51.6000°N 0.1119°W	11.42	263386
13	Harrow	51°35′23″N 0°20′05″W﻿ / ﻿51.5898°N 0.3346°W	19.49	243372
14	Havering	51°34′52″N 0°11′01″E﻿ / ﻿51.5812°N 0.1837°E	43.35	242080
15	Hillingdon	51°32′39″N 0°28′34″W﻿ / ﻿51.5441°N 0.4760°W	44.67	286806
16	Hounslow	51°28′29″N 0°22′05″W﻿ / ﻿51.4746°N 0.3680°W	21.61	262407
17	Islington	51°32′30″N 0°06′08″W﻿ / ﻿51.5416°N 0.1022°W	5.74	215667
18	Kensington and Chelsea	51°30′07″N 0°11′41″W﻿ / ﻿51.5020°N 0.1947°W	4.68	155594
19	Kingston upon Thames	51°24′31″N 0°18′23″W﻿ / ﻿51.4085°N 0.3064°W	14.38	166793
20	Lambeth	51°27′39″N 0°06′59″W﻿ / ﻿51.4607°N 0.1163°W	10.36	314242
21	Lewisham	51°26′43″N 0°01′15″W﻿ / ﻿51.4452°N 0.0209°W	13.57	286180
22	Merton	51°24′05″N 0°11′45″W﻿ / ﻿51.4014°N 0.1958°W	14.52	203223
23	Newham	51°30′28″N 0°02′49″E﻿ / ﻿51.5077°N 0.0469°E	13.98	318227
24	Redbridge	51°33′32″N 0°04′27″E﻿ / ﻿51.5590°N 0.0741°E	21.78	288272
25	Richmond upon Thames	51°26′52″N 0°19′34″W﻿ / ﻿51.4479°N 0.3260°W	22.17	191365
26	Southwark	51°30′13″N 0°04′49″W﻿ / ﻿51.5035°N 0.0804°W	11.14	298464
27	Sutton	51°21′42″N 0°11′40″W﻿ / ﻿51.3618°N 0.1945°W	16.93	195914
28	Tower Hamlets	51°30′36″N 0°00′21″W﻿ / ﻿51.5099°N 0.0059°W	7.63	272890
29	Waltham Forest	51°35′27″N 0°00′48″W﻿ / ﻿51.5908°N 0.0134°W	14.99	265797
30	Wandsworth	51°27′24″N 0°11′28″W﻿ / ﻿51.4567°N 0.1910°W	13.23	310516
31	Westminster	51°29′50″N 0°08′14″W﻿ / ﻿51.4973°N 0.1372°W	8.29	226841
In order to use the Foursquare API, we require a latitude and longitude value for each borough. The co-ordinates column in the above dataframe contains a complex string, so we will format this in order to attain a latitude column and longitiude column containing floats. Below we create a dataframe containing only latitude and longitude values.

In [5]:
df = df.rename(columns={'Co-ordinates': 'Coordinates'})

ll = df.Coordinates.str.split("/").apply(pd.Series) 
ll.columns = ["Delete", "Coordinates"]
ll = ll.drop(ll.columns[0], 1) #Removes the sexagesimal degree co-ordinates

ll = ll.Coordinates.str.split(" ").apply(pd.Series)
ll = ll.drop(ll.columns[0], 1) #Splits Latitude and Longitude into differnt columns 


ll.columns = ["Latitude" , "Longitude"]

ll['Latitude'] = ll['Latitude'].map(lambda x: str(x)[:-2]) 
ll['Longitude'] = ll['Longitude'].map(lambda x: str(x)[:-2])
ll['Latitude'] = ll['Latitude'].str[1:] #Removes characters before and after the co-ordinate values

ll['Longitude'] = ll['Longitude'].astype(float)
ll['Latitude'] = ll['Latitude'].astype(float) #Convert entires from strings to floats

ll['Longitude'] = - ll['Longitude'] 
ll.loc[0, 'Longitude'] = -ll.loc[0, 'Longitude'] 
ll.loc[2, 'Longitude'] = -ll.loc[2, 'Longitude'] 
ll.loc[4, 'Longitude'] = -ll.loc[4, 'Longitude']
ll.loc[9, 'Longitude'] = -ll.loc[9, 'Longitude'] 
ll.loc[14, 'Longitude'] = -ll.loc[14, 'Longitude'] 
ll.loc[23, 'Longitude'] = -ll.loc[23, 'Longitude'] 
ll.loc[24, 'Longitude'] = -ll.loc[24, 'Longitude'] #Manually convert any west longitude values to negative 

ll.head()
Out[5]:
Latitude	Longitude
0	51.5607	0.1557
1	51.6252	-0.1517
2	51.4549	0.1505
3	51.5588	-0.2817
4	51.4039	0.0198
Looking at the original dataframe, we see some of the Borough names contain a "[note ]", we will manually remove these and create a dataframe containing only borough names.

In [6]:
df1 = df.drop(df.columns[1],1)
df1.loc[0, 'Borough'] = 'Barking and Dagenham'
df1.loc[9, 'Borough'] = 'Greenwich'
df1.loc[11, 'Borough'] = 'Hammersmith and Fulham'
df1 = df1.drop('Area (sq mi)' , axis = 1 )
df1 = df1.drop('Population (2013 est)[1]' , axis = 1)

df1.head()
Out[6]:
Borough
0	Barking and Dagenham
1	Barnet
2	Bexley
3	Brent
4	Bromley
Now we are ready to create a final dataframe that will use for analyses. To do this we combine the previous two dataframes and add a population density column. We do this by dividing the population of the borough by area of the borough, this gives us the population per square mile.

In [7]:
df_london = df1
df_london['Latitude'] = ll['Latitude']
df_london['Longitude'] = ll['Longitude']
df_london['Density'] = df['Population (2013 est)[1]']/df['Area (sq mi)']
df_london['Density'] = df_london['Density'].round()

display(df_london)
Borough	Latitude	Longitude	Density
0	Barking and Dagenham	51.5607	0.1557	13952.0
1	Barnet	51.6252	-0.1517	11021.0
2	Bexley	51.4549	0.1505	10123.0
3	Brent	51.5588	-0.2817	18998.0
4	Bromley	51.4039	0.0198	5484.0
5	Camden	51.5290	-0.1255	27348.0
6	Croydon	51.3714	-0.0977	11157.0
7	Ealing	51.5130	-0.3089	15975.0
8	Enfield	51.6538	-0.0799	10098.0
9	Greenwich	51.4892	0.0648	14442.0
10	Hackney	51.5450	-0.0553	34970.0
11	Hammersmith and Fulham	51.4927	-0.2339	28228.0
12	Haringey	51.6000	-0.1119	23064.0
13	Harrow	51.5898	-0.3346	12487.0
14	Havering	51.5812	0.1837	5584.0
15	Hillingdon	51.5441	-0.4760	6421.0
16	Hounslow	51.4746	-0.3680	12143.0
17	Islington	51.5416	-0.1022	37573.0
18	Kensington and Chelsea	51.5020	-0.1947	33247.0
19	Kingston upon Thames	51.4085	-0.3064	11599.0
20	Lambeth	51.4607	-0.1163	30332.0
21	Lewisham	51.4452	-0.0209	21089.0
22	Merton	51.4014	-0.1958	13996.0
23	Newham	51.5077	0.0469	22763.0
24	Redbridge	51.5590	0.0741	13236.0
25	Richmond upon Thames	51.4479	-0.3260	8632.0
26	Southwark	51.5035	-0.0804	26792.0
27	Sutton	51.3618	-0.1945	11572.0
28	Tower Hamlets	51.5099	-0.0059	35765.0
29	Waltham Forest	51.5908	-0.0134	17732.0
30	Wandsworth	51.4567	-0.1910	23471.0
31	Westminster	51.4973	-0.1372	27363.0
Methodology
The individual has two criteria for choosing a borough in London: he wishes to live in a ‘quiet’ area and to be close to multiple coffee shops.

In order to identify which boroughs are ‘quiet’, we will find the population density by dividing the population of the borough by its area in square miles. This will give us the population per square mile.

In order to ensure the borough has multiple coffee shops, we will use the Foursquare API to find all the coffee shops within a 1km radius of the centre of the borough. To do this we will use the coordinates found in our dataframe.

Analysis
Population Density
First we will investigate which Boroughs are 'quieter', in order to do this we will look at the population density. A bar chart comparing the density of all 32 London borough is shown below.

In [8]:
x = df_london['Borough']
y = df_london['Density']
labels = df_london['Borough']

fig, ax = plt.subplots()
bar = ax.bar(x, y)

ax.set_ylim(0,40000)
ax.set_ylabel('Population Density')
ax.set_title('Population Density of London Boroughs')
ax.set_xticklabels(labels , rotation='vertical') #adds vertical labels to each bar


plt.show()

Looking at the bar chart, we see a large variation in population density, ranging from the most dense Islington to the least dense Bromley. For this project we will consider a borough with population density below 10,000 people per square mile to be 'quiet'. We will now create a dataframe containing only locations that fit this criteria.

In [9]:
df_density = df_london[df_london.Density <= 10000]

df_density.head()
Out[9]:
Borough	Latitude	Longitude	Density
4	Bromley	51.4039	0.0198	5484.0
14	Havering	51.5812	0.1837	5584.0
15	Hillingdon	51.5441	-0.4760	6421.0
25	Richmond upon Thames	51.4479	-0.3260	8632.0
Looking at the dataframe, we see there are four boroughs that we classify as 'quiet'.

Next we will investigate which of these boroughs meet the criteria of having lots of coffee shops to choose from. Using the Foursquare API, we will see how many Coffee shops can be found within a 1km radius of the centre of the borough.

First we enter our personal Foursquare details. These have been removed on the Github repositry.

In [10]:
CLIENT_ID = 'V50IUN30HXV4NN03Q5MDIKVSQJROYRW1KHYNIZMN0SK0E4KH'
CLIENT_SECRET = '0S1GVZD1APYRQDOXIBDLEZQTMDLXKHUYDRV2CLIR1KUSHSZY'
VERSION = '20180605'
Bromley
First, we find the latitude and longitude of Bromley.

In [11]:
latitude = round(df_density.loc[4, 'Latitude'] , 2)
longitude = round(df_density.loc[4, 'Longitude'] , 2)
Next, we search all venues within a 1000m or 1km radius of these coordinates.

In [12]:
radius = 1000

url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}'.format(
    CLIENT_ID, 
    CLIENT_SECRET, 
    VERSION, 
    latitude,
    longitude,
    radius, 
    )

results = requests.get(url).json()
To extract the category of the venue we use the following function:

In [13]:
def get_category_type(row):
    try:
        categories_list = row['categories']
    except:
        categories_list = row['venue.categories']
        
    if len(categories_list) == 0:
        return None
    else:
        return categories_list[0]['name']
Now we are ready to clean the json and structure it into a pandas dataframe.

In [14]:
venues = results['response']['groups'][0]['items']
    
nearby_venues = json_normalize(venues) # flatten JSON

filtered_columns = ['venue.name', 'venue.categories', 'venue.location.lat', 'venue.location.lng']
nearby_venues =nearby_venues.loc[:, filtered_columns] # filter columns

nearby_venues['venue.categories'] = nearby_venues.apply(get_category_type, axis=1) # filter the category for each row

nearby_venues.columns = [col.split(".")[-1] for col in nearby_venues.columns]# clean columns

nearby_venues.head()
/Users/ConorSharpe/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: FutureWarning: pandas.io.json.json_normalize is deprecated, use pandas.json_normalize instead
  This is separate from the ipykernel package so we can avoid doing imports until
Out[14]:
name	categories	lat	lng
0	Unico	Ice Cream Shop	51.402189	0.015725
1	wilko	Furniture / Home Store	51.401040	0.016110
2	Marks & Spencer	Department Store	51.404017	0.015718
3	Apple Bromley	Electronics Store	51.402940	0.016252
4	Church House Gardens	Park	51.403010	0.012149
And finally, we create a dataframe containg only venues in the 'Coffee Shop' catergory.

In [15]:
bromley_coffee = nearby_venues[nearby_venues.categories == "Coffee Shop"]

bromley_coffee
Out[15]:
name	categories	lat	lng
10	Caffè Nero	Coffee Shop	51.402653	0.015616
22	Caffè Nero	Coffee Shop	51.400063	0.017092
23	Costa Coffee	Coffee Shop	51.404458	0.016645
24	Costa Coffee	Coffee Shop	51.405653	0.015279
In [16]:
print('Bromley has {} Coffee Shops and a population density of {} per square mile.'.format(len(bromley_coffee) , df_density.iloc[0,3] ))
Bromley has 4 Coffee Shops and a population density of 5484.0 per square mile.
This process is repeated for the other three 'quiet' boroughs.

Havering
In [17]:
latitude = round(float(df_density.loc[14, 'Latitude']) , 2)
longitude = round(float(df_density.loc[14, 'Longitude']) , 2)


radius = 1000
url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}'.format(
    CLIENT_ID, 
    CLIENT_SECRET, 
    VERSION, 
    latitude,
    longitude,
    radius, 
    )

results = requests.get(url).json()


def get_category_type(row):
    try:
        categories_list = row['categories']
    except:
        categories_list = row['venue.categories']
        
    if len(categories_list) == 0:
        return None
    else:
        return categories_list[0]['name']
    
venues = results['response']['groups'][0]['items']
    
nearby_venues = json_normalize(venues) 


filtered_columns = ['venue.name', 'venue.categories', 'venue.location.lat', 'venue.location.lng']
nearby_venues =nearby_venues.loc[:, filtered_columns]


nearby_venues['venue.categories'] = nearby_venues.apply(get_category_type, axis=1)


nearby_venues.columns = [col.split(".")[-1] for col in nearby_venues.columns] 

havering_coffee = nearby_venues[nearby_venues.categories == "Coffee Shop"]

havering_coffee
/Users/ConorSharpe/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:31: FutureWarning: pandas.io.json.json_normalize is deprecated, use pandas.json_normalize instead
Out[17]:
name	categories	lat	lng
0	Costa Coffee	Coffee Shop	51.576890	0.179497
2	Costa Coffee	Coffee Shop	51.576481	0.182448
11	Starbucks	Coffee Shop	51.576281	0.181187
24	Costa Coffee	Coffee Shop	51.579747	0.182072
29	Starbucks	Coffee Shop	51.577947	0.182874
In [18]:
print('Havering has {} Coffee Shops and a population density of {} per square mile.'.format(len(havering_coffee) , df_density.iloc[1,3] ))
Havering has 5 Coffee Shops and a population density of 5584.0 per square mile.
Hillingdon
In [19]:
latitude = round(float(df_density.loc[15, 'Latitude']) , 2)
longitude = round(float(df_density.loc[15, 'Longitude']) , 2)


radius = 1000
url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}'.format(
    CLIENT_ID, 
    CLIENT_SECRET, 
    VERSION, 
    latitude,
    longitude,
    radius, 
    )

results = requests.get(url).json()


def get_category_type(row):
    try:
        categories_list = row['categories']
    except:
        categories_list = row['venue.categories']
        
    if len(categories_list) == 0:
        return None
    else:
        return categories_list[0]['name']
    
venues = results['response']['groups'][0]['items']
    
nearby_venues = json_normalize(venues) 


filtered_columns = ['venue.name', 'venue.categories', 'venue.location.lat', 'venue.location.lng']
nearby_venues =nearby_venues.loc[:, filtered_columns]

nearby_venues['venue.categories'] = nearby_venues.apply(get_category_type, axis=1)

nearby_venues.columns = [col.split(".")[-1] for col in nearby_venues.columns] 

hillingdon_coffee = nearby_venues[nearby_venues.categories == "Coffee Shop"]

hillingdon_coffee
/Users/ConorSharpe/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:31: FutureWarning: pandas.io.json.json_normalize is deprecated, use pandas.json_normalize instead
Out[19]:
name	categories	lat	lng
2	Caffè Nero	Coffee Shop	51.545373	-0.477829
4	Coffee Break	Coffee Shop	51.545263	-0.477433
6	Starbucks	Coffee Shop	51.545742	-0.478584
11	Costa Coffee	Coffee Shop	51.546541	-0.479912
19	Harris + Hoole	Coffee Shop	51.546152	-0.479353
In [20]:
print('Hillingdon has {} Coffee Shops and a population density of {} per square mile.'.format(len(hillingdon_coffee) , df_density.iloc[2,3] ))
Hillingdon has 5 Coffee Shops and a population density of 6421.0 per square mile.
Richmond upon Thames
In [21]:
latitude = round(float(df_density.loc[25, 'Latitude']) , 2)
longitude = round(float(df_density.loc[25, 'Longitude']) , 2)


radius = 1000
url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}'.format(
    CLIENT_ID, 
    CLIENT_SECRET, 
    VERSION, 
    latitude,
    longitude,
    radius, 
    )

results = requests.get(url).json()

def get_category_type(row):
    try:
        categories_list = row['categories']
    except:
        categories_list = row['venue.categories']
        
    if len(categories_list) == 0:
        return None
    else:
        return categories_list[0]['name']
    
venues = results['response']['groups'][0]['items']
    
nearby_venues = json_normalize(venues) 

filtered_columns = ['venue.name', 'venue.categories', 'venue.location.lat', 'venue.location.lng']
nearby_venues =nearby_venues.loc[:, filtered_columns]

nearby_venues['venue.categories'] = nearby_venues.apply(get_category_type, axis=1)

nearby_venues.columns = [col.split(".")[-1] for col in nearby_venues.columns] 

richmond_upon_thames_coffee = nearby_venues[nearby_venues.categories == "Coffee Shop"]

richmond_upon_thames_coffee
/Users/ConorSharpe/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:30: FutureWarning: pandas.io.json.json_normalize is deprecated, use pandas.json_normalize instead
Out[21]:
name	categories	lat	lng
0	The Press Room	Coffee Shop	51.447645	-0.328727
3	Cafe Bellisimo	Coffee Shop	51.447639	-0.328860
17	Harris + Hoole	Coffee Shop	51.446143	-0.328734
In [387]:
print('Richmond upon Thames has {} Coffee Shops and a population density of {} per square mile.'.format(len(richmond_upon_thames_coffee) , df_density.iloc[3,3] ))
Richmond upon Thames has 3 Coffee Shops and a population density of 8632.0 per square mile.
Below we see a scatter plot with population density plotted against number of coffee shops. The boroughs that are 'quiet' and have a low population density appear in the lower right portion of the scatter plot.

In [22]:
x = [len(bromley_coffee), len(havering_coffee) , len(hillingdon_coffee) ,len(richmond_upon_thames_coffee)]
y = df_density['Density']         

fig, ax = plt.subplots()
ax.scatter(x, y)

plt.ylim(4000, 9500)
plt.xlim(2,6)
plt.title('Population Density vs Number of Coffee shops in London Boroughs')
plt.ylabel('Number of people per square mile')
plt.xlabel('Number of Coffee shops within 1000 meters of the Borough centre')

ax.annotate('Hillingdon',
            (5, 6600))

ax.annotate('Richmond Upon Thames',
            (3, 8800))

ax.annotate('Bromley',(4,5700))

ax.annotate('Havering',(5,5800))
Out[22]:
Text(5, 5800, 'Havering')

Looking at the plot, we see the point representing Havering is the closest to the bottom right corner, this is closely followed by the points representing Hillingdon and Bromley.

Another factor to consider is should coffee shops in the same chain (e.g Starbucks) count multiple times in the quantity of coffee shops. Below we investigate how the data looks if we remove these duplicates.

In [23]:
bromley_coffee_distinct = bromley_coffee.drop_duplicates(subset='name',keep='last')

print('Bromley has {} Distinct Coffee Shops and a population density of {} per square mile.'.format(len(bromley_coffee_distinct) , df_density.iloc[0,3] ))

havering_coffee_distinct = havering_coffee.drop_duplicates(subset='name',keep='last')

print('Havering has {} Distinct Coffee Shops and a population density of {} per square mile.'.format(len(havering_coffee_distinct) , df_density.iloc[1,3] ))

hillingdon_coffee_distinct = hillingdon_coffee.drop_duplicates(subset='name',keep='last')

print('Hillingdon has {} Distinct Coffee Shops and a population density of {} per square mile.'.format(len(hillingdon_coffee_distinct) , df_density.iloc[2,3] ))

richmond_upon_thames_coffee_distinct = richmond_upon_thames_coffee.drop_duplicates(subset='name',keep='last')

print('Richmond Upon Thames has {} Distinct Coffee Shops and a population density of {} per square mile.'.format(len(richmond_upon_thames_coffee_distinct) , df_density.iloc[3,3]))
Bromley has 2 Distinct Coffee Shops and a population density of 5484.0 per square mile.
Havering has 2 Distinct Coffee Shops and a population density of 5584.0 per square mile.
Hillingdon has 5 Distinct Coffee Shops and a population density of 6421.0 per square mile.
Richmond Upon Thames has 3 Distinct Coffee Shops and a population density of 8632.0 per square mile.
We will now show this data on a scatter plot.

In [24]:
x = [len(bromley_coffee_distinct), len(havering_coffee_distinct) , len(hillingdon_coffee_distinct) ,len(richmond_upon_thames_coffee_distinct)]
y = df_density['Density']         

fig, ax = plt.subplots()
ax.scatter(x, y)

plt.ylim(4000, 9500)
plt.xlim(0,6)
plt.title('Population Density vs Number of Coffee shops in London Boroughs')
plt.ylabel('Number of people per square mile')
plt.xlabel('Number of Coffee shops within 1000 meters of the Borough centre')

ax.annotate('Hillingdon',
            (5, 6600))

ax.annotate('Richmond Upon Thames',
            (3, 8800))

ax.annotate('Bromley',(2,5150))

ax.annotate('Havering',(2,5800))
Out[24]:
Text(2, 5800, 'Havering')

Here we see Hillingdon is clearly the closests to the desirable region in the bottom right corner. Both Bromley and Havering become less desirable due to containing multiple coffee shops from the same company.

Conclusion
In order to answer the question posed by the individual regarding which London Borough would be most appropriate for him to live in, we first scraped data from a Wikipedia table. Next the data was prepared for analyses using pandas. We then compared the population density of different London Boroughs and used the Foursquare API to find coffee shops within a 1km radius of the centre of the borough. Finally, our findings were plotted on a scatter plot in order to help the individual visualise the information.

If we go by the definition of total number of coffee shops, then we see Havering is the most appropriate borough. This is closely followed by Hillingdon and Bromley. Therefore, we would recommend all three boroughs to the individual, this would allow him to take into account factors not considered in this report and choose his prefered borough.

Looking at the results for when multiple coffee shops of the same chain are counted once, we see Hillingdon is the most appropriate borough. However, this time, it is not closely followed by any other borough. Because of this we would recommend only Hillingdon to the individual.

We would return to the individual with both these conclusions and allow him to make his decision based on them. Should none of the boroughs be appropriate for him, we could increase the cut-off point for population density (ie greater than 10,000 per square mile). This would allow more boroughs to be considered and, potentially, a different conclusion to be made.
